{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPy20cSHIFHL4W99DXLGkSL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ypoxdudeLpNl","executionInfo":{"status":"ok","timestamp":1681502057468,"user_tz":-330,"elapsed":1288,"user":{"displayName":"Kartikay Suri","userId":"02073430634162476827"}},"outputId":"ff5e9b23-5109-4af6-fcfb-56ad852fa657"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement csv (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for csv\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qLAdFOxWLe-u","executionInfo":{"status":"ok","timestamp":1681502905304,"user_tz":-330,"elapsed":87008,"user":{"displayName":"Kartikay Suri","userId":"02073430634162476827"}},"outputId":"c2a283b9-3b7d-474e-ceb4-595d31fe39eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Error: article not found on https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n","Error: article not found on https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n","Error: article not found on https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/\n"]}],"source":["from requests.models import MissingSchema\n","from nltk.downloader import ErrorMessage\n","import requests\n","from bs4 import BeautifulSoup\n","import csv\n","import os\n","\n","# Create a directory to store the scraped data\n","if not os.path.exists('scraped_data'):\n","    os.makedirs('scraped_data')\n","dictval = {}\n","# Read the URLs and IDs from a CSV file\n","with open('/content/Input.csv', 'r') as f:\n","    reader = csv.reader(f)\n","    next(reader)  # Skip the header row\n","    for row in reader:\n","        url_id, url = row\n","        # Send a GET request to the URL\n","        try:\n","          response = requests.get(url.strip())\n","\n","          # Parse the HTML content using Beautiful Soup\n","          soup = BeautifulSoup(response.content, 'html.parser')\n","\n","          # Try to extract the article heading and text\n","          try:\n","              article = soup.find('article')\n","              heading = article.find('h1').get_text().strip()\n","              text = article.find_all(['p'])\n","\n","              # Combine the heading and text into a single string\n","              content = heading + '\\n\\n' + '\\n\\n'.join([tag.get_text().strip() for tag in text])\n","              content = content.encode('ascii', 'ignore').decode('ascii')\n","              # Save the data to a text file with the name of the URL ID\n","              with open(f'scraped_data/{url_id}.txt', 'w', encoding='utf-8') as f:\n","                  f.write(content)\n","              dictval[url_id] = url\n","          except AttributeError:\n","              print(f\"Error: article not found on {url}\")\n","        except MissingSchema:\n","          break\n"]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import os\n","import re\n","import csv\n","# Step 1: Reading the data from a file\n","directory = \"/content/scraped_data\"\n","headers = ['URL_ID', 'URL', 'POSITIVE SCORE', \n","           'NEGATIVE SCORE', 'POLARITY SCORE',\n","           'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH',\n","           'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',\n","           'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT',\n","           'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n","\n","with open('/content/output.csv', mode='w', newline='') as file:\n","  writer = csv.writer(file)\n","  writer.writerow(headers)\n","for filename in os.listdir(directory):\n","    if filename.endswith(\".txt\"):\n","        filepath = os.path.join(directory, filename)\n","        with open(filepath, 'rb') as f:\n","            data = f.read()\n","            data = data.decode('utf-8')\n","\n","            # Step 2: Tokenizing data into words and sentences\n","            words = word_tokenize(data)\n","            sentences = sent_tokenize(data)\n","\n","            # Step 3: Removing stop words and punctuations\n","            stop_words = set()\n","            for word in data:\n","                stop_words.add(word.strip())\n","\n","            # step 4: Calculating Positive and Negative Scores\n","            positive_dict, negative_dict = set(), set()\n","            positive_dict_old = set(open('/content/Master Dictionary/positive-words.txt', 'rb').read().split())\n","            for wordings in positive_dict_old:\n","              try:\n","                wordings = wordings.decode('utf-8')\n","                positive_dict.add(wordings)\n","              except UnicodeDecodeError:\n","                print(f\"Cannot decode item: {wordings}\")\n","\n","            negative_dict_old = set(open('/content/Master Dictionary/negative-words.txt', 'rb').read().split())\n","            for wordings in negative_dict_old:\n","              try:\n","                wordings = wordings.decode('utf-8')\n","                negative_dict.add(wordings)\n","              except UnicodeDecodeError:\n","                print(f\"Cannot decode item: {wordings}\")\n","\n","            words = [w.lower() for w in words if not w.lower() in stop_words and w.isalnum()]\n","\n","            pos_score = sum([1 if w in positive_dict else 0 for w in words])\n","            neg_score = -1 * sum([1 if w in negative_dict else 0 for w in words])\n","\n","            # Step 5: calculating polarity score and subjectivity score\n","            polarity_score = (pos_score - neg_score) / ((pos_score + neg_score) + 0.000001)\n","            subjectivity_score = (pos_score + neg_score) / ((len(words)) + 0.000001)\n","\n","            # Step 6: Calculating Average Number of Words Per Sentence\n","            avg_words_per_sentence = len(words) / len(sentences)\n","\n","            # Step 7: Counting Complex Words\n","            complex_words = [word for word in words if len(word) > 2 and nltk.pos_tag([word])[0][1] in ['NN', 'NNP', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']]\n","            num_complex_words = len(complex_words)\n","            percentage_complex_words = num_complex_words / len(words)\n","\n","            # Step 8: Calculating fog index\n","            fog_index = 0.4 * (avg_words_per_sentence + percentage_complex_words)\n","\n","            # Step 9: Counting Syllables in each word\n","            def count_syllables(word):\n","                vowels = 'aeiouy'\n","                return sum([1 if ch in vowels else 0 for ch in word.lower()])\n","\n","            syllables = [count_syllables(w) for w in words]\n","            num_syllables = sum(syllables)\n","\n","            # Step 10: Counting Personal Pronouns\n","            personal_pronouns = ['i', 'we', 'my', 'ours', 'us']\n","            p_pattern = '|'.join(personal_pronouns)\n","            personals_count = len(re.findall(p_pattern, data.lower()))\n","\n","            # Step 11: Calculating Average Word Length\n","            avg_word_len = sum([len(w) for w in words]) / len(words)\n","            import csv\n","\n","# Create headers for CSV file\n","            \n","# Open CSV file and write headers\n","            \n","\n","            # Example data for writing\n","            url_id = os.path.splitext(filename)[0]\n","            url = dictval[url_id]\n","            # Write data to CSV file\n","            data = [url_id, url, pos_score, neg_score,\n","                    polarity_score, subjectivity_score,\n","                    avg_words_per_sentence, percentage_complex_words,\n","                    fog_index, avg_words_per_sentence, num_complex_words,\n","                    len(words), num_syllables, personals_count, avg_word_len]\n","\n","            with open('/content/output.csv', mode='a', newline='') as file:\n","                writer = csv.writer(file)\n","                writer.writerow(data)\n","            # printing the results for each file\n","            # print(f\"Results for {filename}:\")\n","            # print(\"Positive Score:\", pos_score)\n","            # print(\"Negative Score:\", neg_score)\n","            # print(\"Polarity Score:\", polarity_score)\n","            # print(\"Subjectivity Score:\", subjectivity_score)\n","            # print(\"Average Number of Words Per Sentence:\", avg_words_per_sentence)\n","            # print(\"Complex Word Count:\", num_complex_words)\n","            # print(\"Percentage of Complex Words:\", percentage_complex_words)\n","            # print(\"Fog Index:\", fog_index)\n","            # print(\"Syllable Count Per Word:\", num_syllables)\n","            # print(\"Personal Pronouns Mentioned:\", personals_count)\n","            # print(\"Average Word Length:\", avg_word_len)\n","            # print(\"\\n\")\n","\n","    else:\n","        continue\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fuxqhm8HLhO0","executionInfo":{"status":"ok","timestamp":1681503144505,"user_tz":-330,"elapsed":23070,"user":{"displayName":"Kartikay Suri","userId":"02073430634162476827"}},"outputId":"ac3eb610-a925-4f0c-8b0c-05a091153598"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"output_type":"stream","name":"stdout","text":["Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n","Cannot decode item: b'na\\xefve'\n"]}]},{"cell_type":"code","source":["\n","# Read the CSV file into a list of dictionaries\n","with open('/content/output.csv', mode='r') as csv_file:\n","    csv_reader = csv.DictReader(csv_file)\n","    data = [row for row in csv_reader]\n","\n","# Sort the list of dictionaries by the 'name' field\n","sorted_data = sorted(data, key=lambda row: int(row['URL_ID']))\n","\n","# Write the sorted data back to a CSV file\n","with open('sorted_example.csv', mode='w', newline='') as csv_file:\n","    # fieldnames = ['name', 'age', 'city']  # Replace with your own field names\n","    writer = csv.DictWriter(csv_file, fieldnames=headers)\n","    writer.writeheader()\n","    for row in sorted_data:\n","        writer.writerow(row)\n"],"metadata":{"id":"iZm3st01Z8I9","executionInfo":{"status":"ok","timestamp":1681503427435,"user_tz":-330,"elapsed":384,"user":{"displayName":"Kartikay Suri","userId":"02073430634162476827"}}},"execution_count":41,"outputs":[]}]}